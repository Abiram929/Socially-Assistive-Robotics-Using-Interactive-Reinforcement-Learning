{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387767b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict, Tuple, MultiBinary, MultiDiscrete \n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "from Furhat_Asks import furhat_questions\n",
    "from Furhat_Asks import user_model_one\n",
    "from Furhat_Asks import user_model_two\n",
    "from Furhat_Asks import user_model_three\n",
    "from Furhat_Asks import user_model_four\n",
    "from Furhat_Asks import user_model_five\n",
    "from Furhat_Asks import user_model_six\n",
    "\n",
    "import wandb\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a70803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  2,  3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(np.array([-6, 0, 1]),np.array([50, 2, 4]),dtype=int).sample()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a45076e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiDiscrete([3,3]).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3286818d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceLearningEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, reduce, stay, increase valence/task difficulty\n",
    "        self.action_space = MultiDiscrete([3,3])\n",
    "    \n",
    "        # Set start score\n",
    "        self.score = 0\n",
    "        \n",
    "        # Set length counter 6\n",
    "        self.questions_length = 0\n",
    "        \n",
    "        self.task_difficulty = 1  #start with lowest difficulty\n",
    "        \n",
    "        self.valence = 1  #start with neutral valence\n",
    "        \n",
    "        self.consecutive_bonus = 0\n",
    "              \n",
    "        # observation array previous score/current valence/current task difficulty\n",
    "        self.observation_space = Box(np.array([-6, 0, 0]),np.array([50, 2, 3]),dtype=int)\n",
    "        \n",
    "        self.npdata = npdata = np.array([['Question_number','Valence','Task Difficulty','Agent Reward','Participant Score']])\n",
    "        \n",
    "\n",
    "        \n",
    "    def step(self, action):\n",
    "        #increase questions length by 1\n",
    "        self.questions_length += 1 \n",
    "        \n",
    "        valence_action = action[0] \n",
    "        \n",
    "        task_difficulty_action = action[1]\n",
    "        \n",
    "        #valence action\n",
    "       \n",
    "        if valence_action == 2:\n",
    "            if self.valence <= 1:\n",
    "                self.valence+=1\n",
    "        elif valence_action ==1:\n",
    "            pass\n",
    "        else:\n",
    "            if self.valence >= 1:\n",
    "                self.valence-=1\n",
    "                \n",
    "        #task difficulty action\n",
    "        \n",
    "        if task_difficulty_action == 2:\n",
    "            if self.task_difficulty <= 3:\n",
    "                self.task_difficulty+=1\n",
    "        elif task_difficulty_action == 1:\n",
    "            pass  \n",
    "        else:\n",
    "            if self.task_difficulty >= 2:\n",
    "                self.task_difficulty-=1\n",
    "        \n",
    "        current_sequence, answer = furhat_questions(self.questions_length, self.valence, self.task_difficulty)\n",
    "\n",
    "        #current_sequence, answer = user_model_two(self.questions_length, self.valence, self.task_difficulty)\n",
    "        #current_sequence, answer = user_model_three(self.questions_length, self.valence, self.task_difficulty)\n",
    "        #current_sequence, answer = user_model_four(self.questions_length, self.valence, self.task_difficulty)\n",
    "        #current_sequence, answer = user_model_five(self.questions_length, self.valence, self.task_difficulty)\n",
    "\n",
    "        #current_sequence, answer = user_model_six(self.questions_length, self.valence, self.task_difficulty)\n",
    "        \n",
    "        # Calculate reward and score\n",
    "        previous_score=0\n",
    "        if current_sequence == answer: \n",
    "            self.consecutive_bonus += 1\n",
    "            reward = 5 * self.task_difficulty * self.consecutive_bonus\n",
    "            self.score += 5 * self.task_difficulty\n",
    "            previous_score = 5 * self.task_difficulty\n",
    "        else:            #negative consecutive bonus\n",
    "            self.consecutive_bonus = 0\n",
    "            reward = -1\n",
    "            self.score -= 5\n",
    "            previous_score = -5\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Check if sequence quiz has finished\n",
    "        if self.questions_length >= 6: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "       \n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        \n",
    "        self.npdata=np.append(self.npdata, [[self.questions_length, self.valence, self.task_difficulty, reward, self.score]], axis=0)\n",
    "        \n",
    "        return np.array([previous_score,self.valence,self.task_difficulty]), reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement visuals\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Set start score\n",
    "        self.score = 0\n",
    "        \n",
    "        # Set length counter 6\n",
    "        self.questions_length = 0\n",
    "        \n",
    "        self.task_difficulty = 1  #start with lowest difficulty\n",
    "        \n",
    "        self.valence = 1  #start with neutral valence\n",
    "        \n",
    "        self.consecutive_bonus = 0\n",
    "        \n",
    "        \n",
    "        return np.array([0,self.valence,self.task_difficulty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc6900b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env=SequenceLearningEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02f2c3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a93c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stable_baselines3.common.env_checker import check_env\n",
    "#check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df3041d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WIP\n",
      "SKT\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m      9\u001b[0m     action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m---> 10\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpisode:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Score:\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode, score))\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mSequenceLearningEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_difficulty \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_difficulty\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 55\u001b[0m current_sequence, answer \u001b[38;5;241m=\u001b[39m \u001b[43mfurhat_questions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestions_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_difficulty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#current_sequence, answer = user_model_two(self.questions_length, self.valence, self.task_difficulty)\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m#current_sequence, answer = user_model_three(self.questions_length, self.valence, self.task_difficulty)\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#current_sequence, answer = user_model_four(self.questions_length, self.valence, self.task_difficulty)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Calculate reward and score\u001b[39;00m\n\u001b[0;32m     65\u001b[0m previous_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\OneDrive\\MSc Project - Implementation 2.0\\Furhat_Asks.py:61\u001b[0m, in \u001b[0;36mfurhat_questions\u001b[1;34m(question_no, valence, task_difficulty)\u001b[0m\n\u001b[0;32m     57\u001b[0m title_string\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence Learning Task: Question\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(question_no)\n\u001b[0;32m     59\u001b[0m answer \u001b[38;5;241m=\u001b[39m easygui\u001b[38;5;241m.\u001b[39menterbox(msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mConfirm your answer:\u001b[39m\u001b[38;5;124m'\u001b[39m, title\u001b[38;5;241m=\u001b[39mtitle_string, default\u001b[38;5;241m=\u001b[39manswer, strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 61\u001b[0m answer\u001b[38;5;241m=\u001b[39m\u001b[43manswer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     62\u001b[0m answer\u001b[38;5;241m=\u001b[39manswer\u001b[38;5;241m.\u001b[39mupper()\n\u001b[0;32m     64\u001b[0m correct\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "episodes = 3\n",
    "for episode in range(1, episodes+1):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ede383",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpdf= pd.DataFrame(env.npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7635c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ca0d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b2253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_model_real_learning')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2466a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97137039",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f755c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpdf= pd.DataFrame(env.npdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3bb26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6903d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfpdf.to_excel(\"real_time2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7a4fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d59052",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de6baa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b3c664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5402b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done: \n",
    "        print('info', info)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82682f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6237b0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')\n",
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50f0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3de93",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir={training_log_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d54829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49743039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model, env, n_eval_episodes=3, render=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f505e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
